{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Utils"
      ],
      "metadata": {
        "id": "DWcs3E1A7Lwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Split text into lowercase words. No fancy preprocessing.\n",
        "    \"\"\"\n",
        "    return text.lower().split() #Takes the text and makes it all lower case and splits out each word\n",
        "\n",
        "\n",
        "def build_vocab(docs):\n",
        "    \"\"\"\n",
        "    Build vocabulary from list of docs.\n",
        "    Returns a list of unique words.\n",
        "    \"\"\"\n",
        "    unique_word_list = [] # empty list to store unique words\n",
        "    staging_list = [] # staging list to add all words\n",
        "\n",
        "    for doc in docs: # iterate through the document corpus\n",
        "        int_list = (list(doc.split())) #take each sentence and form its own list\n",
        "        for word in int_list: # take each word from the sentence\n",
        "          staging_list.append(word.lower()) # add that word into the staging list and lower case to match our docs\n",
        "\n",
        "    vocab = list(set(staging_list)) #find all the unique words from the staging list\n",
        "    return vocab # returns a list of unique words from our corpus\n",
        "\n",
        "\n",
        "def text_to_vector(text, vocab):\n",
        "    \"\"\"\n",
        "    Convert text into a vector based on the vocab (Bag of Words).\n",
        "    \"\"\"\n",
        "    tokenized_text = tokenize(text) # tokenize the input text\n",
        "    vector_list = [] # empty list to store vectors\n",
        "    for word in vocab: # vocab is our documents (corpus)\n",
        "      count = tokenized_text.count(word) # count how many times each word occurs\n",
        "      vector_list.append(count) # append the counts of each word into the list\n",
        "    return vector_list #return the list.\n",
        "\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between two vectors.\n",
        "    \"\"\"\n",
        "    a = np.array(v1) # convert into array\n",
        "    b = np.array(v2)\n",
        "    dot_product = np.dot(a, b) #find the dot product of the two vectors\n",
        "    norm_a = np.linalg.norm(a) # normalize\n",
        "    norm_b = np.linalg.norm(b)\n",
        "    return dot_product / (norm_a * norm_b) # compute cosine similarity"
      ],
      "metadata": {
        "id": "Zmr-_Sk72dQ8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Store"
      ],
      "metadata": {
        "id": "5XSnhGAx87EQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class VectorStore:\n",
        "    def __init__(self, documents):\n",
        "        self.documents = documents\n",
        "        self.vocab = build_vocab(documents) # this will create a list of all unique words from our documents\n",
        "        self.doc_vectors = [text_to_vector(doc, self.vocab) for doc in documents] # create text to vector for each doc in documents\n",
        "\n",
        "    def search(self, query, k=3):\n",
        "        \"\"\"\n",
        "        Search for top-k documents similar to the query.\n",
        "        \"\"\"\n",
        "        query_vector = text_to_vector(query, self.vocab) # vectorize the inout query by using our utils\n",
        "        #print(f\"The query vectors is: {query_vector}\")\n",
        "        similarity_score = [] # empoty list to store scores and index\n",
        "        for doc_number, each_doc_vector in enumerate(self.doc_vectors): # get the doc number and doc vector\n",
        "          score = cosine_similarity(query_vector, each_doc_vector) # create the cosine similarity sccore\n",
        "          similarity_score.append((float(score), doc_number)) # add the score and doc number into list\n",
        "        similarity_score.sort(key=lambda x:x[0], reverse=True)\n",
        "        return similarity_score[:k] # return the top k score and doc number.\n",
        "\n"
      ],
      "metadata": {
        "id": "iHm09p4fCKuZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "iq9J837g9AdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# docs below is our internal knowledge base, this could be anything of your choice.\n",
        "docs = [\n",
        "    \"AI is the future\",\n",
        "    \"I love programming in Python\",\n",
        "    \"Artificial intelligence and machine learning are related fields\",\n",
        "    \"Python is popular for data science\",\n",
        "    \"AI is the most interesting pltform in the world\"\n",
        "]\n",
        "\n",
        "my_vector_db = VectorStore(docs)\n",
        "list_a = my_vector_db.search(query=\"Which language do I love?\", k=2) # query is the user question, feel free to adjust this. K is the number or documents to retrieve\n",
        "print(list_a)\n",
        "best_results = list_a[0][1] # we only print the document with the highest matching result.\n",
        "print(f\"The closest matching document is: {docs[best_results]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rwqZ_lPLCLa7",
        "outputId": "59caee50-7593-4662-8f30-89f9d42880bd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love programming in Python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our simple RAG system.\n",
        "\n",
        "###Tokenize:\n",
        "####This will take a piece of text and break it into individual tokens. For case sensitivity all tokens will be lowercase. For example \"AI is the future\" becomes ['ai','is','the','future',]\n",
        "\n",
        "###build_vocab:\n",
        "####This function will take a coprus of documents (docs) and create a list of unique words from the entire corpus. Additional info about each line can be found in the comments.\n",
        "\n",
        "###text-to-vector:\n",
        "####Take a string of text and convert it into vectors based on how many times the word occurs in the sentence.\n",
        "\n",
        "###cosine_similarity:\n",
        "####compute the cosine similarity of two vectors (user_question, knowledge_base_docs_vector)\n",
        "\n",
        "###VecgorStore\n",
        "####This is the final class that will use all the tools defined above. Here I take a user_question and perform the following steps\n",
        "\n",
        "###tokenize -> text_to_vector -> cosine smilarity against our corpus -> print the top-k documents with the highest similarity score.\n"
      ],
      "metadata": {
        "id": "v_L2EVcz9g5-"
      }
    }
  ]
}